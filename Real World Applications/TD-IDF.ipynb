{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def read_stopwords(file_path):\n",
    "    return set(word.strip() for word in open(file_path))\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8') # required to convert to unicode\n",
    "\n",
    "stop_words = read_stopwords('stop_words_en.txt')\n",
    "word_dictionary = {}\n",
    "i = 0;\n",
    "article = None\n",
    "word_count = 0\n",
    "current_word = None\n",
    "total_words = 0\n",
    "for line in sys.stdin:\n",
    "    if i > 0:\n",
    "        break\n",
    "    try:\n",
    "        article, text = unicode(line.strip()).split('\\t', 1)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "    words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "    total_words = 0\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word = word.strip().lower()\n",
    "        if word:\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            new_words.append(word)\n",
    "   \n",
    "    total_words = len(new_words)\n",
    "    new_words.sort()\n",
    "    word_count = 0;\n",
    "    current_word = None\n",
    "\n",
    "    for word in new_words:\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        if current_word != word:\n",
    "            if current_word:\n",
    "                tf = float(word_count)/total_words\n",
    "                print (\"%s\\t%s\\t%f\" % (current_word, article, tf))\n",
    "            current_word = word\n",
    "            word_count = 0;\n",
    "        word_count += 1\n",
    "        \n",
    "if current_word:\n",
    "    tf = float(word_count/total_words)\n",
    "    print (\"%s\\t%s\\t%f\" % (current_word, article, tf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "current_article = None\n",
    "current_word = None\n",
    "article_count = 0\n",
    "word_dictionary = {}\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        word, article, tf = line.strip().split('\\t', 2)\n",
    "        tf = float(tf)\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "        \n",
    "    if current_word != word:\n",
    "        if current_word:\n",
    "            idf = float(1)/math.log(1 + article_count)\n",
    "            for a in word_dictionary:\n",
    "                tff = float(word_dictionary[a])\n",
    "                print (\"%s\\t%s\\t%f\\t%f\\t%f\" % (current_word, a, tff, idf, tff*idf))\n",
    "        word_dictionary.clear()\n",
    "        article_count = 0\n",
    "        current_word = word\n",
    "        \n",
    "    article_count += 1;\n",
    "    word_dictionary[article] = tf\n",
    "    \n",
    "if current_word:\n",
    "    idf = float(1)/math.log(1 + article_count)\n",
    "    for a in word_dictionary:\n",
    "        tf = float(word_dictionary[a])\n",
    "        print (\"%s\\t%s\\t%f\\t%f\\t%f\" % (current_word, a, tf, idf, tf*idf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/07/20 16:01:27 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/07/20 16:01:28 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "18/07/20 16:01:30 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "18/07/20 16:01:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "18/07/20 16:01:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1532067686572_0020\n",
      "18/07/20 16:01:30 INFO impl.YarnClientImpl: Submitted application application_1532067686572_0020\n",
      "18/07/20 16:01:31 INFO mapreduce.Job: The url to track the job: http://38df42ab9577:8088/proxy/application_1532067686572_0020/\n",
      "18/07/20 16:01:31 INFO mapreduce.Job: Running job: job_1532067686572_0020\n",
      "18/07/20 16:01:40 INFO mapreduce.Job: Job job_1532067686572_0020 running in uber mode : false\n",
      "18/07/20 16:01:40 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "18/07/20 16:02:05 INFO mapreduce.Job: Task Id : attempt_1532067686572_0020_m_000000_0, Status : FAILED\n",
      "Container [pid=21010,containerID=container_1532067686572_0020_01_000002] is running beyond virtual memory limits. Current usage: 359.9 MB of 1 GB physical memory used; 3.6 GB of 2.1 GB virtual memory used. Killing container.\n",
      "Dump of the process-tree for container_1532067686572_0020_01_000002 :\n",
      "\t|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE\n",
      "\t|- 21028 21010 21010 21010 (java) 368 35 1912520704 45994 /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1532067686572_0020/container_1532067686572_0020_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1532067686572_0020/container_1532067686572_0020_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 44889 attempt_1532067686572_0020_m_000000_0 2 \n",
      "\t|- 21010 21008 21010 21010 (bash) 0 0 11546624 158 /bin/bash -c /opt/jdk/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx200m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/jovyan/appcache/application_1532067686572_0020/container_1532067686572_0020_01_000002/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/opt/hadoop/logs/userlogs/application_1532067686572_0020/container_1532067686572_0020_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.17.0.2 44889 attempt_1532067686572_0020_m_000000_0 2 1>/opt/hadoop/logs/userlogs/application_1532067686572_0020/container_1532067686572_0020_01_000002/stdout 2>/opt/hadoop/logs/userlogs/application_1532067686572_0020/container_1532067686572_0020_01_000002/stderr  \n",
      "\t|- 21083 21028 21010 21010 (java) 0 0 1912520704 45994 N/A\n",
      "\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "18/07/20 16:02:07 INFO mapreduce.Job:  map 4% reduce 0%\n",
      "18/07/20 16:02:15 INFO mapreduce.Job:  map 9% reduce 0%\n",
      "18/07/20 16:02:37 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "18/07/20 16:02:46 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "18/07/20 16:02:47 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "18/07/20 16:02:57 INFO mapreduce.Job:  map 34% reduce 0%\n",
      "18/07/20 16:02:58 INFO mapreduce.Job:  map 42% reduce 0%\n",
      "18/07/20 16:03:09 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "18/07/20 16:03:11 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "18/07/20 16:05:35 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "18/07/20 16:05:37 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "18/07/20 16:06:18 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "18/07/20 16:06:29 INFO mapreduce.Job:  map 100% reduce 38%\n",
      "18/07/20 16:07:05 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "18/07/20 16:07:06 INFO mapreduce.Job:  map 100% reduce 46%\n",
      "18/07/20 16:07:08 INFO mapreduce.Job:  map 100% reduce 63%\n",
      "18/07/20 16:07:13 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "18/07/20 16:07:30 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "18/07/20 16:07:31 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "18/07/20 16:07:34 INFO mapreduce.Job: Job job_1532067686572_0020 completed successfully\n",
      "18/07/20 16:07:35 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2014058\n",
      "\t\tFILE: Number of bytes written=5431080\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=76874501\n",
      "\t\tHDFS: Number of bytes written=3375034\n",
      "\t\tHDFS: Number of read operations=30\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=9\n",
      "\t\tOther local map tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=172836\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1390344\n",
      "\t\tTotal time spent by all map tasks (ms)=172836\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1390344\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=172836\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1390344\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=176984064\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1423712256\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4100\n",
      "\t\tMap output records=85064\n",
      "\t\tMap output bytes=1843882\n",
      "\t\tMap output materialized bytes=2014106\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7446\n",
      "\t\tReduce shuffle bytes=2014106\n",
      "\t\tReduce input records=85064\n",
      "\t\tReduce output records=85064\n",
      "\t\tSpilled Records=170128\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=63869\n",
      "\t\tCPU time spent (ms)=89050\n",
      "\t\tPhysical memory (bytes) snapshot=1007104000\n",
      "\t\tVirtual memory (bytes) snapshot=19316412416\n",
      "\t\tTotal committed heap usage (bytes)=385536000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=76874273\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=3375034\n",
      "18/07/20 16:07:35 INFO streaming.StreamJob: Output directory: wordcount_result\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUT_DIR=\"wordcount_result_\"$(date +\"%s%6N\")\n",
    "NUM_REDUCERS=8\n",
    "\n",
    "hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null\n",
    "\n",
    "yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapred.jab.name=\"Streaming wordCount\" \\\n",
    "    -D mapreduce.job.reduces=${NUM_REDUCERS} \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -files mapper1.py,reducer1.py,/datasets/stop_words_en.txt \\\n",
    "    -mapper \"python mapper1.py\" \\\n",
    "    -reducer \"python reducer1.py\" \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -input /data/wiki/en_articles_part \\\n",
    "    -output ${OUT_DIR} > /dev/null \n",
    "\n",
    "hdfs dfs -cat ${OUT_DIR}/part* | grep -w \"labor\" | grep -w \"12\" | cut -f 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
